

===================== Infracture Management ===============================

** How do we manage infrastructure in the cloud with all the benefits it gives us? How do we handle its highly dynamic nature? The answer came in the form of vendor-specific tools like CloudFormation or agnostic solutions like Terraform. When combined with tools that allow us to create images, they represent a new generation of configuration management. We are talking about full automation backed by immutability.

** We are talking about full automation backed by immutability

** Today, modern infrastructure is created from immutable images. Any upgrade is performed by building new images and performing rolling updates that will replace VMs one by one. Infrastructure dependencies are never changed at runtime. Tools like Packer, Terraform, CloudFormation, and the like are the answer to today’s problems.

** One of the inherent benefits behind immutability is a clear division between infrastructure and deployments. Until not long ago, the two meshed together into an inseparable process. With infrastructure becoming a service, deployment processes can be clearly separated, thus allowing different teams, individuals, and expertise to take control.



Tools:
-----
Terraform (HashiCorp)
Packer (HashiCorp)
Cloudformation (AWS)

===================== Deployment Processes ===============================

** The need of the hour, We needed process isolation that does not require a separate VM for each service. At the same time, we had to come up with an immutable way to deploy software

** Linux got namespaces, cgroups, and other things that are together known as containers. They were lightweight, fast, and cheap. They provided process isolation and quite a few other benefits.


====================== The Scheduler =======================================

* K8s make sure the desired and actual state of deployment is same
* For k8s all containers are same, they don't care what is inside.
* K8s scheduler takes container as basic unit.


=============== what is k8s ============================

* If we need to operate containers at scale, be fault tolerant and self-healing, and have the other features we expect from modern clusters, we need more. We need at least a scheduler, probably more
* Kubernetes was first developed by a team at Google. It is based on their experience from running containers at scale for years. Later on, it was donated to Cloud Native Computing Foundation (CNCF). It is a true open source project with probably the highest velocity in history.

Why Kubernetes?

	Let’s discuss how Kubernetes is not only a container scheduler but a lot more.
	We can use it to deploy our services, to roll out new releases without downtime, and to scale (or de-scale) those services.
	It is portable.
	It can run on a public or private cloud.
	It can run on-premise or in a hybrid environment.
	We can move a Kubernetes cluster from one hosting vendor to another without changing (almost) any of the deployment and management processes.
	Kubernetes can be easily extended to serve nearly any needs. We can choose which modules we’ll use, and we can develop additional features ourselves and plug them in.
	Kubernetes will decide where to run something and how to maintain the state we specify.
	Kubernetes can place replicas of a service on the most appropriate server, restart them when needed, replicate them, and scale them.
	Self-healing is a feature included in its design from the start. On the other hand, self-adaptation is coming soon as well.
	Zero-downtime deployments, fault tolerance, high availability, scaling, scheduling, and self-healing add significant value in Kubernetes.
	We can use it to mount volumes for stateful applications.
	It allows us to store confidential information as secrets.
	We can use it to validate the health of our services.
	It can load balance requests and monitor resources.
	It provides service discovery and easy access to logs.


------------------------------------ POD ----------------------------------------------------

* From the Kubernetes’ perspective, there’s nothing smaller than a Pod.
* A Pod encapsulates one or more containers. It provides a unique network IP, attaches storage resources, and also decides how containers should run. Everything in a Pod is tightly coupled
* We’ll create a Kubernetes cluster using k3d, the first command will create a k3d cluster named mycluster.
	k3d cluster create mycluster
	kubectl get nodes
* Running simple POD in a cluster
	kubectl run db --image mongo
	kubectl get pods
	docker exec -it k3d-mycluster-server-0 ctr container ls | grep mongo 
	kubectl delete pod db	

* POD through declarative syntax
	- API refernce guide
	- Commands
		kubectl create -f db.yml
		kubectl get pods
		kubectl get pods -o wide
		kubectl get pods -o json
		kubectl get pods -o yaml
		kubectl describe pod db	
		kubectl get events
	- Containers are only ever created within the context of a Pod. This is usually done using a Controller. See Controllers: Deployment, Job, or StatefulSet
	- Manifest is a doc that contains resources info that is used for creating resources by using different k8s API end-points	
	- The describe sub-command returned details of the specified resource. In this case, the resource is the Pod named db.

* Component and stage involved in POD's scheduling:
	Sequential breakdown of events
		The sequence of events that transpired with the kubectl create -f db.yml command is as follows:
		Kubernetes client (kubectl) sent a request to the API server requesting creation of a Pod defined in the db.yml file.
		Since the scheduler is watching the API server for new events, it detected that there is an unassigned Pod.
		The scheduler decided which node to assign the Pod to and sent that information to the API server.
		Kubelet is also watching the API server. It detected that the Pod was assigned to the node it is running on.
		Kubelet sent a request to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines a single container based on the mongo image.
		Finally, Kubelet sent a request to the API server notifying it that the Pod was created successfully.

* Playing around with the Running Pod

* Running Multiple Containers in a Single Pod
	- All the processes (containers) inside a Pod share the same set of resources, and they can communicate with each other through localhost. One of those shared resources is storage.

* Single vs. Multi-Container Pods
	- A Pod is a collection of containers that share the same resources. Not much more. Everything else should be accomplished with higher-level constructs
	- A Pod is a collection of containers. However, that does not mean that multi-container Pods are common. They are rare. Most Pods you’ll create will be single container units.	
	- Most of the scenarios where you might think that multi-container Pod is a good solution will probably be solved through other resources.
	- A frequent use case is multi-container Pods used for:
			Continuous integration (CI)
			Continuous Delivery (CD)
			Continuous Deployment processes (CDP)		
	
* Monitoring Health
	- Liveness Probe
		* livenessProbe can be used to confirm whether a container should be running. If the probe fails, Kubernetes will kill the container and apply restart policy which defaults to Always.
		* livenessProbe:
          httpGet:
            path: /this/path/does/not/exist
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 2 # Defaults to 1
          periodSeconds: 5 # Defaults to 10
          failureThreshold: 1 # Defaults to 3

* pod and container 
	- pod is a group of one or more containers with shared storage/network and a specification for how to run the containers
	- Container created from image with the purpose to host an application

----------------------------------------- Relicaset -------------------------------------------------

* ReplicaSet
	- ReplicaSet’s primary function is to ensure that the specified number of replicas of service are (almost) always running.
	- If you’re familiar with Replication Controllers, it is worth mentioning that ReplicaSet is the next-generation ReplicationController. The only significant difference is that ReplicaSet has extended support for selectors. Everything else is the same. ReplicationController is considered deprecated, so we’ll focus only on ReplicaSet.

* creating replicaSets:	
	- selector is used to match and ensure number of pods (replicas) always running that is matching spec.template.metadata.labels
	- The exact number of replicas running, could have been created by any process, not restricted to current deployment.
	- The apiVersion, kind, and metadata fields are mandatory with all Kubernetes objects. ReplicaSet is no exception, i.e., it is also a Kubernetes object.

* Sequential Breakdown of the Process:
		-Kubernetes client (kubectl) sent a request to the API server requesting the creation of a ReplicaSet defined in the go-demo-2.yml file.
		-The controller is watching the API server for new events, and it detected that there is a new ReplicaSet object.
		-The controller creates two new pod definitions because we have configured replica value as 2 in go-demo-2.yml file.
		-Since the scheduler is watching the API server for new events, it detected that there are two unassigned Pods.
		-The scheduler decided which node to assign the Pod and sent that information to the API server.
		-Kubelet is also watching the API server. It detected that the two Pods were assigned to the node it is running on.
		-Kubelet sent requests to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines two containers based on the 	mongo and api image. So in total four containers are created.
		-Finally, Kubelet sent a request to the API server notifying it that the Pods were created successfully.

* operating replicaSets:
		- ReplicaSets and Pods are loosely coupled through matching labels and that ReplicaSets are using those labels to maintain the parity between the actual and the desired state. So far, self-healing worked as expected.
		- However, since ReplicaSets and Pods are loosely coupled objects with matching labels, we can remove one without deleting the other.
		- what is the outcome when replicaset deleted but pods are not
			* when replicaset will be deleted, the containers will still be running. replicaset is the object that tracks matching number of pods to replicas
			* ReplicaSet uses labels to decide whether the desired number of Pods is already running in the cluster, should lead us to the conclusion that if we create the same ReplicaSet again, it should reuse the two Pods that are running in the cluster.
		- The good news is that ReplicaSets are relatively straightforward. They provide a guarantee that the specified number of replicas of a Pod will be running in the system as long as there are available resources. That’s the primary and, arguably, the only purpose.
		- The bad news is that ReplicaSets are rarely used independently. You will almost never create a ReplicaSet directly just as you’re not going to create Pods. Instead, we tend to create ReplicaSets through Deployments. In other words, we use ReplicaSets to create and control Pods, and Deployments to create ReplicaSets (and a few other things). We’ll get to Deployment soon.

	
* Quiz
		- When compared with the replication Controllers, ReplicaSets have an extended support for? 
		Ans: Selectors
		- Which of the following fields are mandatory when defining a Kubernetes object?
		Ans: apiVersion , kind, metadata
		- Which of the following argument is used to prevent Kubernetes from removing the downstream objects?
		Ans: --cascade=false
		- Which of the following is the key property of ReplicaSets?	
		Ans : Self-healing
		- ReplicaSets make sure that?
		Ans: The desired number of Pods are running

--------------------------------------------- Services -------------------------------------------------------

* Getting Started with Communication
		- Kubernetes Services provide addresses through which associated Pods can be accessed.
		- We can use the kubectl expose command to expose a resource as a new Kubernetes Service. That resource can be a Deployment, another Service, a ReplicaSet, a ReplicationController, or a Pod. We’ll expose the ReplicaSet since it is already running in the cluster.

			* kubectl expose rs go-demo-2 \
	    --name=go-demo-2-svc \
	    --target-port=28017 \
	    --type=NodePort

* creating services by exposing ports
	  - other types of services
	  	* NodePort
	  		- If NodePort is used, ClusterIP will be created automatically.
	  	* ClusterIP
	  		- ClusterIP is useful when we want to enable communication between Pods and still prevent any external access.
	  	* LoadBalancer
	  		- The LoadBalancer type is only useful when combined with cloud provider’s load balancer.
	  	* ExternalName  


* Sequential Breakdown of the Process
		- Kubernetes client (kubectl) sent a request to the API server requesting the creation of the Service based on Pods created through the go-demo-2 ReplicaSet.

		- Endpoint controller is watching the API server for new service events. It detected that there is a new Service object.

		- Endpoint controller created endpoint objects with the same name as the Service, and it used Service selector to identify endpoints (in this case the IP and the port of go-demo-2 Pods).

		- kube-proxy is watching for service and endpoint objects. It detected that there is a new Service and a new endpoint object.

		- kube-proxy added iptables rules which capture traffic to the Service port and redirect it to endpoints. For each endpoint object, it adds iptables rule which selects a Pod.

		- The kube-dns add-on is watching for Service. It detected that there is a new service.

		- The kube-dns added db's record to the dns server (skydns).

		commands
	       kubectl apply -f go-demo-2-rs.yml
         kubectl get rs
         kubectl get pods
         kubectl get svc
         kubectl expose rs go-demo-2  --name=go-demo-2-svc --target-port=28017 --type=NodePort
         kubectl get svc
         kubectl describe svc/go-demo-2-svc
         kubectl get pods -owide


* Creating Services through Declarative Syntax
		- kubectl get ep go-demo-2 -o yaml (describe endpoint)
		- We can see that there are two subsets, corresponding to the two Pods that contain the same labels as the Service selector.
		- Each Pod has a unique IP that is included in the algorithm used when forwarding requests. Actually, it’s not much of an algorithm. Requests will be sent to those Pods randomly. That randomness results in something similar to round-robin load balancing. If the number of Pods does not change, each will receive an approximately equal number of requests.
		- However soon, when the newer Kubernetes versions get released, we’ll have an alternative to the iptables solution. We’ll be able to apply different types of load balancing algorithms like last connection, destination hashing, newer queue, and so on
		- apiVersion: v1
			kind: Service
			metadata:
			  name: go-demo-2
			spec:
			  type: NodePort
			  ports:
			  - port: 28017
			    nodePort: 30001
			    protocol: TCP
			  selector:
			    type: backend
			    service: go-demo-2

* Splitting the Pod and Establishing communication through Services
		- If there is no type in service spec, so it’ll default to ClusterIP.
			* The same is true for the protocol. TCP is all we need, and it happens to be the default one.	
		
		  apiVersion:  apps/v1
			kind: ReplicaSet
			metadata:
			  name: go-demo-2-db
			spec:
			  selector:
			    matchLabels:
			      type: db
			      service: go-demo-2
			  template:
			    metadata:
			      labels:
			        type: db
			        service: go-demo-2
			        vendor: MongoLabs
			    spec:
			      containers:
			      - name: db
			        image: mongo:3.3
			        ports:
			        - containerPort: 28017		
			-----------------------------
			apiVersion: v1
			kind: Service
			metadata:
			  name: go-demo-2-db
			spec:
			  ports:
			  - port: 27017
			  selector:
			    type: db
			    service: go-demo-2		        

* Creating the Split API Pods
		- service name should be used between service and db pods for communications
		- While livenessProbe is used to determine whether a Pod is alive or it should be replaced by a new one, the readinessProbe is used by the iptables. A Pod that does not pass the readinessProbe will be excluded and will not receive requests. In theory, requests might be still sent to a faulty Pod, between two iterations. Still, such requests will be small in number since the iptables will change as soon as the next probe responds with HTTP code less than 200, or equal or greater than 400
		- Commands:
				kubectl get all
						* It will display k8s objects (pods , services, replicasets etc. )
				kubectl apply -f go-demo-2-db-rs.yml 
   			kubectl apply -f go-demo-2-db-svc.yml 
   			kubectl apply -f go-demo-2-api-rs.yml 
				   			kubectl apply -f go-demo-2-api-svc.yml 
   			kubectl get all
   			nohup kubectl port-forward service/go-demo-2-api --address 0.0.0.0  3000:8080 > /dev/null 2>&1 &
   			curl -i "0.0.0.0:3000/demo/hello"
   			curl -i "http://localhost:3000/demo/hello"
   		  kubectl get ep
   		  kubectl get svc go-demo-2-api -o yaml

   	- Access service using shell:	  
   		  PORT=$(kubectl get svc go-demo-2-svc \
				    -o jsonpath="{.spec.ports[0].nodePort}")

				IP=$(minikube ip)

				curl -i "http://$IP:$PORT/demo/hello"

* Defining Multiple Objects in the Same YAML file
		- For simplicity we can keep pods/service together in a single yml file.The only difference is that each object definition is separated by three dashes (---).


* Discovering Services
			- Every Pod gets environment variables for each of the active Services. They are provided in the same format as what Docker links expect, as well with the simpler Kubernetes-specific syntax.

			- Services can be discovered through two principal modes:

						* Environment variables
								- kubectl exec $POD_NAME -- env
								- The last two environment variables are Kubernetes specific and follow the [SERVICE_NAME]_SERVICE_HOST and [SERVICE_NAME]_SERIVCE_PORT format (service name is upper-cased).

						* DNS
								- kubectl describe svc go-demo-2-db
								- The key is in the IP field. That is the IP through which this service can be accessed and it matches the values of the environment variables GO_DEMO_2_DB_* and GO_DEMO_2_DB_SERVICE_HOST.
			
		- Sequential breakdown of the process:
				Let’s go through the sequence of events related to service discovery and components involved.

				- When the api container go-demo-2 tries to connect with the go-demo-2-db Service, it looks at the nameserver configured in /etc/resolv.conf. kubelet configured the nameserver with the kube-dns Service IP (10.96.0.10) during the Pod scheduling process.

				- The container queries the DNS server listening to port 53. go-demo-2-db DNS gets resolved to the service IP 10.0.0.19. This DNS record was added by kube-dns during the service creation process.

				- The container uses the service IP which forwards requests through the iptables rules. They were added by kube-proxy during Service and Endpoint creation process.

				- Since we only have one replica of the go-demo-2-db Pod, iptables forwards requests to just one endpoint. If we had multiple replicas, iptables would act as a load balancer and forward requests randomly among Endpoints of the Service.		




--------------------------------------------- Deployments -------------------------------------------------------

* It shows that Deployments control ReplicaSets. The Deployment created the ReplicaSet which, in turn, created Pods.
		- Deployment uses spec.containers to generate replicasets which in turn creates and manages pods

* deployment vs replicaset -> The real advantage of Deployments becomes evident if we try to change some of its aspects. For example, we might choose to upgrade MongoDB to version 3.4.

* We will regularly add --record to the kubectl create commands. This allows us to track each change to our resources such as a Deployments.

Sequential Deployment steps:
	- Kubernetes client (kubectl) sent a request to the API server requesting the creation of a Deployment defined in the go-demo-2-db.yml file.
	- The deployment controller is watching the API server for new events, and it detected that there is a new Deployment object.
	- 	The deployment controller creates a new ReplicaSet object.

Updating Deployments:
	 - pods hash : You’ll notice that contained within the name of the Pod is a hash which matches the hash in the name of the new ReplicaSet, namely “6b48fcbfcf”. Even though it might look like it is a random value, it is not.If you destroy the Deployment and create it again, you’ll notice that the hash in the Pod name and ReplicaSet name remain consistent. This value is generated by hashing the PodTemplate of the ReplicaSet. As long as the PodTemplate is the same, the hash value will be the same as well.That way a Deployment can know whether anything related to the Pods has changed and, if it does, will create a new ReplicaSet.

Defining a Zero-Downtime Deployment:
		- Zero-downtime deployment is a prerequisite for higher frequency releases.

		- revisionHistoryLimit :	 It defines the number of old ReplicaSets we can rollback. Like most of the fields, it is set to the sensible default value of 10. We changed it to 5 and, as a result, we will be able to rollback to any of the previous five ReplicaSets.
		
		- The strategy can be either the RollingUpdate or the Recreate type
				* Would there be an adverse effect if two different versions of my application are running in parallel? If that’s the case, a Recreate strategy might be a good choice and you must be aware that you cannot accomplish zero-downtime deployments.The Recreate strategy is much better suited for our single-replica database

				* The RollingUpdate strategy is the default type, for a good reason. It allows us to deploy new releases without downtime. It creates a new ReplicaSet with zero replicas and, depending on other parameters, increases the replicas of the new one, and decreases those from the old one. The process is finished when the replicas of the new ReplicaSet entirely replace those from the old one.


Creating a Zero-Downtime Deployment:
		-  The process started by increasing the number of replicas of the new ReplicaSet (“go-demo-2-api-559d6888df”) to 1. Next, it decreased the number of replicas of the old ReplicaSet (“go-demo-2-api-566c5b97f5”) to 2. The same process of increasing replicas of the new, and decreasing replicas of the old ReplicaSet continued until the new one got the desired number (3), and the old one dropped to zero.
		There was no downtime throughout the process. Users would receive a response from the application no matter whether they sent it before, during, or after the update. The only important thing is that, during the update, a response might have come from the old or the new release. During the update process, both releases were running in parallel.

		- One negetive usecase is , if there is any issue in new image being rolled out, it will be in crachloop/imagepull error and the older replica will be reduced to 0.

		- sample commands
			kubectl create -f go-demo-2-api.yml --record
			kubectl set image -f go-demo-2-api.yml api=vfarcic/go-demo-2:2.0
			kubectl rollout status -w -f go-demo-2-api.yml
			kubectl rollout history -f go-demo-2-api.yml


Rolling Back or Rolling Forward?:
		* Rolling back a release that introduced database changes is often not possible. Even when it is, rolling forward is usually a better option when practicing continuous deployment with high-frequency releases limited to a small scope of changes.

		* 'set image' for dynamic deployement and 'rollout undo' for rollback

		Commands:
			* kubectl set image -f go-demo-2-api.yml api=vfarcic/go-demo-2:3.0 --record
			* kubectl rollout undo -f go-demo-2-api.yml
			* kubectl rollout history -f go-demo-2-api.yml
			* kubectl get all

Playing around with the Deployment:
		* we can rollout to any previous revision

		commands:
			kubectl set image -f go-demo-2-api.yml api=vfarcic/go-demo-2:3.0 --record
			kubectl set image -f go-demo-2-api.yml api=vfarcic/go-demo-2:4.0 --record
			kubectl rollout history -f go-demo-2-api.yml
			kubectl rollout undo -f go-demo-2-api.yml --to-revision=2


Rolling Back Failed Deployments:
		*  Please make sure that at least 60 seconds have passed since you executed the kubectl set image command. If you’re wondering why we are waiting, the answer lies in the progressDeadlineSeconds field set in the go-demo-2-api Deployment definition. That’s how much the Deployment has to wait before it deduces that it cannot progress due to a failure to run a Pod

		kubectl set image -f go-demo-2-api.yml api=vfarcic/go-demo-2:does-not-exist --record
		echo $?
		kubectl rollout undo -f go-demo-2-api.yml
		kubectl rollout status -f go-demo-2-api.ym

Merging Everything into the Same YAML Definition:
		* deployment and services are merged

Updating Multiple Objects:
		* kubectl get deployments --show-labels
		* kubectl get deployments -l type=db,vendor=MongoLabs
		* kubectl set image deployments -l type=db,vendor=MongoLabs db=mongo:3.4 --record
				- It will match all eployment with thoes labels and update the image	
		* kubectl describe -f go-demo-2.yml
				- verify the image version					

Scaling Deployments:		
		* Similarly, we’ll use kubectl scale to change the number of replicas. Consider this an introduction to automation that is coming later on.
			- kubectl scale deployment go-demo-2-api --replicas 8 --record
			- kubectl scale deployment go-demo-2-api --replicas 5 --record

		* we can scale up / down the replicas
		* NUmber of deployements / replicasets match the number of replicas



--------------------------------------------- Ingress -------------------------------------------------------

Getting Started with Ingress:
		* It seems that we already accomplished the external access to the applications through Kubernetes Services, they do not make the applications truly accessible. We still need forwarding rules based on paths and domains, SSL termination and a number of other features 
		* In a more traditional setup, we’d probably use an external proxy and a load balancer. Ingress provides an API that allows us to accomplish these things, in addition to a few other features we expect from a dynamic cluster.


Enabling Ingress Controllers:
	  * Kubernetes itself does not have a ready-to-go solution for this. Unlike other types of Controllers that are typically part of the kube-controller-manager binary, Ingress Controller needs to be installed separately. Instead of a Controller, kube-controller-manager offers Ingress resource that other third-party solutions can utilize to provide requests forwarding and SSL features. In other words, Kubernetes only provides an API, and we need to set up a Controller that will use it.

	  *  The ingress image is based on NGINX Ingress Controller. It is one of the only two currently supported and maintained by the Kubernetes community. The other one is GLBC that comes with Google Compute Engine (GCE) Kubernetes hosted solution.

	  * By default, the Ingress Controller is configured with only two endpoints. If we’d like to check Controller’s health, we can send a request to /healthz.			nohup kubectl port-forward -n ingress-nginx service/ingress-nginx-controller 3000:80 --address 0.0.0.0 > /dev/null 2>&1 &

				# Please wait for a few seconds before running the next command 
				curl -i "0.0.0.0:3000/healthz"

Creating Ingress Resources Based on Paths:
		* annotations: The specification API defines only the fields that are mandatory for all Ingress Controllers. All the additional info an Ingress Controller needs is specified through annotations.
			- We specified the annotation nginx.ingress.kubernetes.io/ssl-redirect: "false" which tells the Controller that we do NOT want to redirect all HTTP requests to HTTPS

		* We specified a set of rules in the spec section. They are used to configure Ingress resources. For now, our rule is based on http with a single path and a backend. All the requests with the path starting with /demo will be forwarded to the service go-demo-2-api on the port 8080.	
		* sample:
				nohup kubectl port-forward -n ingress-nginx service/ingress-nginx-controller 3000:80 --address 0.0.0.0 > /dev/null 2>&1 &
				curl -i "http://0.0.0.0:3000/demo/hello"


Sequential Breakdown of the Process:
		* The Kubernetes client (kubectl) sent a request to the API server requesting the creation of the Ingress resource defined in the go-demo-2.yml file.
		* The ingress controller is watching the API server for new events. It detected that there is a new Ingress resource.
		* The ingress controller configured the load balancer. In this case, it is nginx which modified nginx.conf with the values of all go-demo-2-api endpoints.

		* Sample Code:
			nohup kubectl port-forward -n ingress-nginx service/ingress-nginx-controller 3000:80 --address 0.0.0.0  > /dev/null 2>&1 &
			#Now click on the link beside the run button 
			#Next run the following command
			curl "http://0.0.0.0:3000/demo/hello"						

Creating Ingress Resources Based on Domains:
		* We can also configure ingress resource using domain (host:)	
		* Sample:
				kubectl apply -f devops-toolkit-dom.yml --record
				kubectl get svc -n ingress-nginx
				nohup kubectl port-forward -n ingress-nginx service/ingress-nginx-controller 3000:80 --address 0.0.0.0 > /dev/null 2>&1 &
				curl -i "http://0.0.0.0:3000"
				curl -I -H "Host: devopstoolkitseries.com" "http://0.0.0.0:3000"
				curl -H "Host: acme.com" "http://0.0.0.0:3000/demo/hello"		

		* Snaps :
			root@3379b2971102d1f0:/usercode# kubectl get svc -n ingress-nginx
			NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
			ingress-nginx-controller-admission   ClusterIP      10.43.232.92    <none>        443/TCP                      8m3s
			ingress-nginx-controller             LoadBalancer   10.43.104.145   172.19.0.3    80:30907/TCP,443:32502/TCP   8m3s
			
			root@3379b2971102d1f0:/usercode# kubectl get pods -n ingress-nginx
			NAME                                        READY   STATUS      RESTARTS   AGE
			svclb-ingress-nginx-controller-d4gdx        2/2     Running     0          8m47s
			ingress-nginx-admission-create-24f42        0/1     Completed   0          8m47s
			ingress-nginx-controller-55dcf56b68-vtll4   1/1     Running     0          8m47s			

Creating an Ingress Resource with Default Backend:
		* When an Ingress spec is without rules, it is considered a default backend. As such, it will forward all requests that do not match paths and/or domains set as rules in the other Ingress resources.

		* when a two default ingress is created, one with functional and other with non-functional service endpoint than for each request only the default ingress with functional service will be selected.

		* sample:
					kubectl create -f default-backend.yml
					nohup kubectl port-forward -n ingress-nginx service/ingress-nginx-controller 3000:80 --address 0.0.0.0 > /dev/null 2>&1 &
					curl -I -H "Host: acme.com" "http://0.0.0.0:3000"
					k3d cluster delete mycluster --all


Comparison with Docker Swarm:
		* Kubernetes provides a well-defined Ingress API that third-party solutions can utilize to deliver a seamless experience. Behind this Ingress resource could be nginx, voyager, haproxy, or trafficserver Ingress Controller. All of them use the same Ingress API to deduce which Services should be used by forwarding algorithms

		* Docker Swarm does not have anything resembling an Ingress API



--------------------------------------------- Volumes -------------------------------------------------------		

Getting Started with Volumes:
	* There are over twenty-five Volume types supported by Kubernetes.
	* many Volume types are specific to a hosting vendor. For example, awsElasticBlockStore works only with AWS, azureDisk and azureFile work only with Azure, and so on and so forth.


Accessing Host’s Resources through hostPath Volumes:
	* hostPath allows us to mount a file or a directory from a host to Pods and, through them, to containers
	* Docker consists of two main pieces. There is a client, and there is a server. When we executed docker image ls, we invoked the client which tried to communicate with the server through its API. The problem is that Docker server is not running in that container. What we should do is tell the client (inside a container) to use Docker server that is already running on the host.By default, the client sends instructions to the server through the socket located in “/var/run/docker.sock”. We can accomplish our goal if we mount that file from the host into a container.
			
			* kubectl run docker --image=docker:17.11  --restart=Never docker image ls

	* manifest sample:

			spec:
		    containers:
		    - name: docker
		      image: docker:17.11
		      command: ["sleep"]
		      args: ["100000"]
		      volumeMounts:
		      - mountPath: /var/run/docker.sock
		        name: docker-socket
		    volumes:
		    - name: docker-socket
		      hostPath:
		        path: /var/run/docker.sock
		      type: Socket	

		   Getting Error: 
		   
		   	Warning  FailedMount  23s (x10 over 4m33s)  kubelet            MountVolume.SetUp failed for volume "docker-socket" : hostPath type check failed: /var/run/docker.sock is not a socket file
  			Warning  FailedMount  16s (x2 over 2m30s)   kubelet            Unable to attach or mount volumes: unmounted volumes=[docker-socket], unattached volumes=[docker-socket kube-api-access-jjxpj]: timed out waiting for the condition  

  * Types of hostpath:
  		- The Directory type will mount a directory from the host. It must exist on the given path. If it doesn’t, we might switch to DirectoryOrCreate type which serves the same purpose. The difference is that DirectoryOrCreate will create the directory if it does not exist on the host.

			- The File and FileOrCreate are similar to their Directory equivalents. The only difference is that this time we’d mount a file, instead of a directory.

			- The other supported types are Socket, CharDevice, and BlockDevice. They should be self-explanatory. If you don’t know what character or block devices are, you probably don’t need those types.    

Running the Pod after mounting hostPath:
		* docker and many other images use alpine as the base
		* If you’re not familiar with alpine, it is a very slim and efficient base image, and we strongly recommend that you use it when building your own.
		* Images like debian, centos, ubuntu, redhat, and similar base images are often a terrible choice made because of a misunderstanding of how containers work.
	  * Sample commands:

	  	kubectl create -f hostpath-mounting.yaml
			kubectl exec -it docker -- docker image ls --format "{{.Repository}}"
			kubectl exec docker -it -- sh
			apk add -U git
			git clone https://github.com/Faizan-Zia/go-demo-2
			cd go-demo-2
			docker image build -t vfarcic/go-demo-2:beta .
			docker image ls --format "{{.Repository}}"
			docker system prune -f
			docker image ls --format "{{.Repository}}"
			exit
			kubectl delete -f hostpath-mounting.yaml

		* usecases:
				- hostpath mounting can be used to build docker images across different nodes using pods, where artifact required to build image can be pulled into init pods and images will be created in nodes and once completed pods will be discarded.

Using hostPath Volume Type to Inject Configuration Files:
		* Prometheus needs a full external-url if we want to change the base path.
		* Changing Prometheus configuration: 
				We could, for example, enter the container, update the configuration file, and send the reload request to Prometheus. That would be a terrible solution since it would last only until the next time we update the application, or until the container fails, and Kubernetes decides to reschedule it.Let’s explore alternative solutions. We could, for example, use hostPath Volume for this as well. If we can guarantee that the correct configuration file is inside the cluster, the Pod could attach it to the prometheus container. Let’s try it out. The output of the prometheus-hostpath with relevant segments is shown below.